---
layout: post
title: Python 爬虫实战--笔记
date: 2020-08-30
Author: 小萌 
tags: [学习案例,技术分享]
comments: true
toc: true
---

> 简单的记录一下，近期来学习Python 的成果。这里，自学了Python基础和抓包实战。

视频地址：https://blaclacloud.coding.net/p/tcshare/files/all

------

### 首先简单描述一下，抓包实现的功能。

1. 首先，根据python，利用request 包，获取整个页面内的目标链接。

2. 其次，利用第一步收获的url数组，for 循环来处理每个页面。

3. 其后，再for 循环内，抓取目标块，将其保存到本地。

这里，我们获取了html的内容，将其全部直接保存成 .html 文件，方便直接预览。

### 代码

```python
##导入依赖
import requests
import time
import re
from bs4  import BeautifulSoup

#获取页面上的links
def get_links(url):
	responce=requests.get(url)
	soup=BeautifulSoup(responce.text,'lxml')
	link_div=soup.find('div',class_='news-list-col')
	links_all=link_div.find_all('a')
	links_get=[a.get('href') for a in links_all]
	links=links_get[10:]
	#### 得到所有的最新链接--不是完整的。links_get[10:]去除前面的10条广告链接
	return links

#获取单页面的内容
def get_title_all(url):
	responce=requests.get('host{}'.format(url))
	soups=BeautifulSoup(responce.text,'lxml') 
	result=re.search('article', url)
	link_title_dow=''
	link_title=''
	link_title_img=''
	if(result):  ####匹配两种模式的页面
		link_title=soups.find('div',class_='inform-title-txt')
	else:
		link_title_but=soups.find('div',class_='inform-button')		
		link_title_dow=link_title_but.find('a')
		link_title=soups.find('div',class_='inform-title')	
		link_title_img=soups.find('div',class_='inform-img')	
	#将页面的标题、下载链接、标题图片连接在一起
    link='{}<br>{}<br>{}<br>'.format(link_title,link_title_dow,link_title_img)
	return link
	#### 解析网页页面,获取标题

def get_connect_all(url):
	responce=requests.get('https://www.kjsv.com{}'.format(url))
	soups=BeautifulSoup(responce.text,'lxml') 
	link_div=soups.find('div',class_='inform-intr')
	return link_div
	#### 解析网页页面,获取内容块


now_name=time.strftime("%m-%d", time.localtime()) 
#保存位置 时间+站点
file_path='./get_html/{}-kjsv.html'.format(now_name)
print('创建文件位置{}'.format(file_path))

links_all=get_links('host')
#开始循环
for link in links_all:
	link_con_a=str(get_connect_all(link))
	link_title_a=str(get_title_all(link))
	link_con=link_con_a.replace("/download/","//host/download/")
    link_title=link_title_a.replace("/download/","//host/download/")
	#保存单次获取的内容保存
	with open(file_path,'a+',encoding='UTF-8',errors='ignore') as f:
		contents=f.write('<br><br><hr><b>'+str(link_title)+'</b>\n')
		contents=f.write(str(link_con)+'\n<hr>')
	print('输出一条信息')

```

### 思路与解析

首先，整个小项目包含2个大函数，对应3个小函数。

| 函数 | 目的 |
| ---- | ---- |
|  get_links    | 获取整个页面上所有有用的链接，通常是首页 |
|  get_connect_all    | 获取子页面的文本内容 |
|  get_title_all    | 获取子页面的标题 |

这里，get_connect_all和 get_title_all本质上一致，但是由于案例项目的特殊性，采取单获取标题和内容。

#### python 代码方法解析

```python
def get_links(url):
    #请求requests.get方法，请求整个页面的html代码
	responce=requests.get(url) 
    #BeautifulSoup 解析html
	soup=BeautifulSoup(responce.text,'lxml')
    #soup.find 查找元素及类
	link_div=soup.find('div',class_='news-list-col')
    #.find_all 获取所有的a标签代码，包含href内容
	links_all=link_div.find_all('a')
    #for 循环清理href ，获取单链接
	links_get=[a.get('href') for a in links_all]
	return links
```

这里，着重讲一下 soup.find 和 soup.find_all 的区别。

soup.find：

​		获取该字段内的第一个特征元素，一般适用于获取页面 html 的唯一一个、独一无二的元素。id值和class按需求填写

| class写法 | find('div',class_='news-list-col') |
| :-------- | ---------------------------------- |
| id写法    | find('div',id='news-list')         |

​	这里，记得find前面的soup格式为lxml

soup.find_all

​		获取字段内的所有满足条件的值，返回数组的形式。那么，得到的结果是字典格式。

### python的爬虫并不难，重要的是爬虫获取信息的时候，确保无误

